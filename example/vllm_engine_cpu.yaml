args:
  model: meta-llama/Meta-Llama-3.1-8B-Instruct
  port: 8000
  tensor-parallel-size:
    # - 1
    - 2
    - 4
  block-size:
    - 8
    - 16
    - 32
  # distributed-executor-backend: mp
  # swap-space:
  #   - 4
  #   - 16
  # max-num-batched-tokens:
  #   - 8192
  #   - 16384
  max-num-seqs:
    - 64
    - 128
    - 256
  # rope-scaling: 
  #   - '{"rope_type":"llama3","factor":1.0, "low_freq_factor": 1.0, "high_freq_factor": 4.0, "original_max_position_embeddings": 8192}'
  #   - '{"rope_type":"llama3","factor":2.0, "low_freq_factor": 1.0, "high_freq_factor": 4.0, "original_max_position_embeddings": 8192}'
  num-scheduler-steps:
    - 1
    - 2
    - 4
  scheduler-delay-factor:
    - 0.0
    - 0.5
    - 1.0
  max-model-len: 8192


envs:
  VLLM_CPU_KVCACHE_SPACE:
    - 80
  #   - 16
  #   - 40
  #   - 80
  VLLM_ATTENTION_BACKEND: TORCH_SDPA
  VLLM_CPU_OMP_THREADS_BIND: '0-53|56-109'

run_config:
  mean_input_tokens: 128
  mean_output_tokens: 256
  num_concurrent_requests: 
    - 100
    # - 20